# yaml-language-server: $schema=./json_schema/sibytes_yetl_pipeline_schema.json

version: 2.0.2
# paths here are relative to this configuration document
tables: ./tables.yaml

audit_control:
  delta_lake:
    # delta table properties can be set at stage level or table level
    delta_properties:
        delta.appendOnly: true
        delta.autoOptimize.autoCompact: true
        delta.autoOptimize.optimizeWrite: true
    managed: false
    container: datalake
    location: /mnt/{{container}}/data
    path: "{{database}}/{{table}}"
    options: null

landing:
  read:
    # trigger: customerdetailscomplete-{{filename_date_format}}*.flg
    trigger_type: file
    database: "{{database}}"
    table: "{{table}}"
    container: landing
    location: "/mnt/{{container}}/data/{{project}}/csv/{{table.replace('_','-')}}/{{path_date_format}}"
    filename: "{{table.replace('_','-')}}-{{filename_date_format}}*.csv"
    filename_date_format: "%Y%m%d"
    path_date_format: "%Y%m%d"
    format: csv
    spark_schema: "../schema/{{table.lower()}}.yaml"
    options:
      # schema
      inferSchema: false
      enforceSchema: true
      columnNameOfCorruptRecord: _corrupt_record
      # csv
      header: true
      mode: PERMISSIVE
      encoding: utf-8
      delimiter: "|"
      escape: '"'
      nullValue: ""
      quote: '"'
      emptyValue: ""
    

raw:
  delta_lake:
    database: "{{database}}"
    table: "{{table}}"
    container: datalake
    location: /mnt/{{container}}/data
    path: "{{database}}/{{table}}"
    z_order_by:
      - _load_date
    options:
      mergeSchema: true
        # optional leave it out if you don't want them
    warning_thresholds:
      invalid_rows: 0
    # optional leave it out if you don't want them
    exception_thresholds:
      invalid_rows: 0


base:
  delta_lake:
    database: "{{database}}"
    table: "{{table}}"
    container: datalake
    location: /mnt/{{container}}/data
    path: "{{database}}/{{table}}"
    options: null
